\documentclass{article}
\usepackage{amsmath,amssymb,amsthm,tikz,tkz-graph,color,chngpage,soul,hyperref,csquotes,graphicx,floatrow,listings, mathrsfs,framed,scrextend,mathrsfs,mathtools,setspace}
\usepackage[shortlabels]{enumitem}
\usepackage[nobreak=true]{mdframed}
\usepackage[margin=0.75in]{geometry}
\usetikzlibrary{matrix,calc}\MakeOuterQuote{"}
\renewenvironment{leftbar}[2][\hsize]{
\def\FrameCommand
{
    {\color{#2}\vrule width 3pt}
    \hspace{0pt}
    }
    \MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}
}
{\endMakeFramed}
\newtheorem*{prop}{Proposition}
\renewcommand{\theenumi}{\alph{enumi}}
\newtheorem{theorem}{Theorem}
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\N}{\mathbb N}
\newcommand{\x}[1]{\textrm{#1}}
\newcommand{\pr}[1]{\textrm{Pr}[#1]}
\newcommand{\xs}[1]{\textrm{ #1 }}
\newcommand{\dpic}[1]{\begin{center}\includegraphics[width=0.5\textwidth]{#1}\end{center}}
\newcommand{\dpicl}[1]{\\\includegraphics[width=0.5\textwidth]{#1}\\}
\newcommand{\tpic}[1]{\begin{center}\includegraphics[width=0.33\textwidth]{#1}\end{center}}
\newcommand{\wpic}[1]{\begin{center}\includegraphics[width=0.8\textwidth]{#1}\end{center}}
\newcommand{\sumlim}[3]{\sum\limits_{#1}^{#2}#3}
\newcommand{\eq}[1]{\begin{equation}#1\end{equation}}
\newcommand{\eqx}[1]{\begin{equation*}#1\end{equation*}}
\newcommand{\w}{\omega}\newcommand{\Om}{\Omega}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\easy}[2]{\begin{leftbar}{#1}#2\end{leftbar}}
\newcommand{\eqs}[1]{\begin{mdframed}#1\end{mdframed}}
\newcommand{\simple}[1]{\easy{gray}{\begin{enumerate}[1.]#1\end{enumerate}}}
\newcommand{\ex}[1]{\easy{blue}{#1}}
\providecommand{\abs}[1]{\lvert#1\rvert} \providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\items}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\itemss}[2]{\begin{itemize}[#1]#2\end{itemize}}
\newcommand{\rng}[2]{#1,\ldots,#2}
\newcommand{\E}[1]{E[#1]}
\newcommand{\var}[1]{\x{Var}[#1]}
\newcommand{\e}{\varepsilon}
\newcommand{\limty}{\lim_{n\rightarrow\infty}}
\newcommand{\ninfty}{n\rightarrow\infty}
\newcommand{\cov}{\x{cov}}
\newmdenv[topline=false, rightline=false, bottomline=false,%
  linewidth=3pt, innerrightmargin=0pt, leftmargin=4pt,%
  innerleftmargin=5pt, skipabove=5pt, skipbelow=5pt]{mdleftbar}
\newcommand{\example}[2]{\textbf{Example: }\\#1\begin{mdleftbar}\onehalfspacing{#2}\end{mdleftbar}}
\newcommand{\cur}{\mathscr}
\newcommand{\bmat}[1]{\begin{bmatrix*}[r]#1\end{bmatrix*}}
\newcommand{\bmatc}[1]{\begin{bmatrix*}[c]#1\end{bmatrix*}}
\newcommand{\bmats}[2]{\begin{bmatrix}[#1]#2\end{bmatrix}}
\newcommand{\case}[2]{\eq{#1=\begin{cases}#2\end{cases}}}
\newcommand{\intlim}[2]{\int_{#1}^{#2}}

\title{CS70 - Lecture 27 Notes}
\author{Name: Felix Su$\quad$SID: 25794773}
\date{Spring 2016$\quad$GSI: Gerald Zhang}
\begin{document}
\maketitle

\subsection*{Review: Continuous Probability}
\simple{
	\item \textbf{pdf}: $\pr{X \in (x,x +\delta]} = f_X (x)\delta$.
	\items{
	    \item Probability of point in $\Omega$ = 0, so define prob. of events as intervals = pdf($\delta$)
	    \item Pdf is non-negative and integrates to 1
	}
	\item \textbf{CDF}: $\pr{X \le x} = F_X (x) =\intlim{-\infty}{x}f_X (y)dy$.
	\items{
	    \item $\pr{a < x \le b}= \pr{X \le b} - \pr{X \le a}$
	}
	\item \textbf{U[a,b], Expo($\lambda$), target}
	\item \textbf{Expectation}: $\E{X} =\intlim{-\infty}{\infty}xf_X (x)dx$.
	\items{
	    \item $x*$probability of $X=x$ in that interval
	}
	\item \textbf{Expectation of function}: $\E{h(X)} =\intlim{-\infty}{\infty}h(x)f_X (x)dx$.
	\item \textbf{Variance}: $\var{X} = \E{(X-\E{X})^2} = \E{X^2}-\E{X}^2$.
	\item \textbf{Gaussian}: $N (\mu,\sigma^2) : f_X (x) = \ldots$ "bell curve"
	\items{
	    \item When you add up many small RVs, the CDF comes out as a bell shape
	}
}

\subsection*{Normal Distribution}
\tpic{gauss.png}
\items{
	\item For any mean: $\mu$ and std. dev: $\sigma$, a \textbf{normal} (aka \textbf{Gaussian}) random variable $Y$, which we write as $Y = \cur{N}(\mu,\sigma^2)$, has pdf $f_Y (y) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)^2/2\sigma^2}$
	\item Standard normal has $\mu = 0$ and $\sigma = 1$.
	\item Note: $\pr{\abs{Y - \mu} > 1.65\sigma} = 10\%$;$\pr{\abs{Y - \mu} > 2\sigma} = 5\%$.
	\item Guassian RV is within $2\sigma$ of the mean with $95\%$
}
\eqs{
\textbf{Normal Distribution}\\
For any $\mu$ and $\sigma$, a Gaussian RV, $Y = \cur{N}(\mu,\sigma^2)$ has pdf:
\eq{f_Y (y) = \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)^2/2\sigma^2}}
}

\subsection*{Scaling and Shifting}
\simple{
    \item If you scale a Gaussian RV (by $Y = \mu +\sigma X$), you get another Gaussian RV
    \item When you multiply at RV by a constant ($\sigma$), you multiply its variance by the square of that constant ($\sigma^2$)
}
\items{
	\item \textbf{Theorem} Let $X = \cur{N} (0,1)$ and $Y = \mu +\sigma X$. Then
	\items{
	    \item $Y = \cur{N} (\mu,\sigma^2)$.
	}
	\item \textbf{Proof:} $f_X (x) = \frac{1}{\sqrt{2\pi}}\x{exp}\set{-\frac{x^2}{2}}$.
	\items{
		\item Now, $f_Y (y) =\frac{1}{\sigma}f_X (\frac{y - \mu}{\sigma})$ (See Lec. 26, slide 19.)
		\item $=\frac{1}{\sqrt{2\pi\sigma2}}\x{exp}\set{-\frac{(y - \mu)^2}{2\sigma^2}}$
	}
}

\subsection*{Expectation, Variance}
\items{
	\item \textbf{Theorem} If $Y = \cur{N} (\mu,\sigma^2)$, then
	\items{
	    \item $\E{Y} = \mu$ and $\var{Y} = \sigma^2$
	}
	\item \textbf{Proof:} It suffices to show the result for $X =\cur{N}(0,1)$ since $Y = \mu +\sigma X,\ldots$
	\item Thus, $f_X (x) = \frac{1}{2\pi}\x{exp}\set{-\frac{x^2}{2}}$.
	\items{
		\item First note that $\E{X} = 0$, by symmetry.
		\item $\var{X} = \E{X^2} =\int x^2\frac{1}{\sqrt{2\pi}}\x{exp}\set{-\frac{x^2}{2}}dx$
		\item $= -\frac{1}{\sqrt{2\pi}}\int xd \x{exp}\set{-\frac{x^2}{2}}$
		\item $=\frac{1}{\sqrt{2\pi}}\int \x{exp}\set{-\frac{x^2}{2}}dx$ (Integration by Parts:$\intlim{a}{b}fdg = [fg]^b_a -\intlim{a}{b}gdf$)
		\item $= \int f_X (x)dx = 1$
	}
}
\subsection*{Central Limit Theorem}
\simple{
    \item Tells us how many samples to take in order for the airthmetic mean to tend to the expected value of an RV
    \item Derive the Normalized Sample mean $S_n =\frac{A_n - \mu}{\sigma/\sqrt{n}}=\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}$
    \item \textbf{CLT:} As $\ninfty$, the distribution of $S_n \rightarrow$ the standard normal distribution $\cur{N}(0,1)$
    \items {
        \item Expectation of $S_n$ is always 0
        \item Variance of $S_n$ is always 1 
    }
}
\items{
    \item \textbf{Law of Large Numbers:} For any set of independent identically distributed random variables, $X_i$, $A_n =\frac{1}{n} \sum X_i$ "tends to the mean."
    \items{
        \item Say $X_i$ have expectation $\mu = \E{X_i}$ and variance $\sigma^2$.
        \item Mean of $A_n$ is $\mu$, and variance is $\sigma^2/n$.
        \item Let $S_n =\frac{A_n - \mu}{\sigma/\sqrt{n}}=\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}$
        \item Then, $\E{S_n} = \frac{1}{\sigma/\sqrt{n}}(\E{An}- \mu) = 0$
        \item $\var{S_n} =\frac{1}{\sigma^2/n}\var{A_n} = 1$.
    }
    \item \textbf{Central limit theorem:} As $n$ goes to infinity the distribution of $S_n$ approaches the standard normal distribution.
    \items{
        \item Expectation of $S_n$ is always 0
        \item Variance of $S_n$ is always 1
    }
}
\textbf{Central Limit Theorem}
\eqs{
\textbf{Normalized Sample Mean}
\eq{S_n =\frac{A_n - \mu}{\sigma/\sqrt{n}}=\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}}
\textbf{Expected Value of Normalized Sample Mean}
\eq{\E{S_n} = \frac{1}{\sigma/\sqrt{n}}(\E{An}- \mu) = 0}
\textbf{Variance of Normalized Sample Mean}
\eq{\var{S_n} =\frac{1}{\sigma^2/n}\var{A_n} = 1}
}
\subsection*{Central Limit Theorem}
\items{
    \item  Let $X_1,X_2,\ldots$ be i.i.d. with $\E{X_1} = \mu$ and $\var{X_1} = \sigma^2$.
    \item Define $S_n := \frac{A_n - \mu}{\sigma/\sqrt{n}}=\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}$.
    \items{
        \item Then, $S_n \rightarrow\cur{N}(0,1)$,as $\ninfty$.
        \item That is, $\pr{S_n \le \alpha} \rightarrow\frac{1}{\sqrt{2\pi}}\intlim{-\infty}{\alpha}e^{-x^2/2}dx$.
        \item \textbf{Proof:} See EE126.
    }
    \item THe CDF of the RV $S_n$ approches the CDF of $\cur{N}(0,1)$
    \items{
        \item PDF begins to look like a bell shape
        \item CDF looks like the integral of a bell shape
    }
}
\eqs{
\textbf{Central Limit Theorem:}
\eq{\pr{S_n \le \alpha} \rightarrow\frac{1}{\sqrt{2\pi}}\intlim{-\infty}{\alpha}e^{-x^2/2}dx}
}
\subsection*{Confidence Interval (CI) for Mean}
\items {
    \item Let $X_1,X_2,\ldots$ be i.i.d. with mean $\mu$ and variance $\sigma^2$ and $A_n =\frac{X_1 +\cdots+X_n}{n}$.
    \item The CLT states that $\frac{A_n - \mu}{\sigma/\sqrt{n}}=\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}\rightarrow\cur{N}(0,1)$ as $\ninfty$.
    \items {
        \item Thus, for $n >> 1$, one has $\pr{\abs{\frac{A_n-\mu}{\sigma/\sqrt{n}}} \le 2} \approx 95\%$.
        \item Equivalently, $\pr{\mu \in [A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]} \approx 95\%$.
        \item That is, $[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]$ is a 95\% -CI for $\mu$.
    }
}
\eqs{
\textbf{Confidence Interval (CI) for Mean}
\eq{\pr{\abs{\frac{A_n-\mu}{\sigma/\sqrt{n}}} \le 2} \approx 95\%}
\eq{[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}] \xs{is a 95\% -CI for} \mu}
}
\items {
    \item Let $X_1,X_2,\ldots$ be i.i.d. with mean $\mu$ and variance $\sigma^2$ and $A_n = \frac{X_1 +\cdots+X_n}{n}$.
    \item The CLT states that $\frac{X_1 +\cdots+X_n -n\mu}{\sigma\sqrt{n}}\rightarrow\cur{N}(0,1)$ as $\ninfty$.
    \items {
        \item Also, $[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]$ is a 95\% -CI for $\mu$.
        \item Recall: Using Chebyshev, we found that (see Lec. 22, slide 6)
        \item $[A_n-4.5\frac{\sigma}{\sqrt{n}},A_n+4.5\frac{\sigma}{\sqrt{n}}]$ is a 95\% -CI for $\mu$.
        }
    \item Thus, the CLT provides a smaller confidence interval.
    \items {
        \item Chebyshev works for all values of $n$.
        \item The CLT assumes $n$ is large enough.
    }
}
\subsection*{Example Question: Question like this will be on the FINAL}
\example{Coins and normal}
{
Let $X_1,X_2,\ldots$ be i.i.d. $B(p)$. Thus, $X_1 +\cdots+X_n = B(n,p)$.\\
Here, $\mu = p$ and $\sigma = \sqrt{p(1-p)}$.\\
CLT states that $\frac{X_1 +\cdots+X_n -np}{\sqrt{p(1-p)n}}\rightarrow\cur{N}(0,1)$ and $[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]$ is a 95\% -CI for $\mu$ with $A_n = (X_1 +\cdots+X_n)/n$.\\
Hence, $[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]$ is a 95\% -CI for $p$.\\
Solve $\frac{d\var{X}}{dp}=\frac{d\sigma^2}{dp}$, test each result to find the $p$ that returns the max of the Variance\\
Substitute the returned $p$ back in to solve for the max value of $\sigma$\\
Since $\sigma \le 0.5$,
Substitute $\sigma$ with the upper bound: $[A_n-2\frac{0.5}{\sqrt{n}},A_n+2\frac{0.5}{\sqrt{n}}]$ is a 95\% -CI for $p$.\\
Thus, $[A_n-\frac{1}{\sqrt{n}},A_n+\frac{1}{\sqrt{n}}]$ is a 95\% -CI for $p$.
}
\subsection*{Summary: Gaussian and CLT}
\simple{
	\item \textbf{Gaussian:} $\cur{N}(\mu,\sigma^2) : f_X (x) = \ldots$ "bell curve"
	\item \textbf{CLT:} $X_n$ i.i.d. $\implies \frac{A_n-\mu}{\sigma/\sqrt{n}}\rightarrow\cur{N}(0,1)$
	\item \textbf{CI:} $[A_n-2\frac{\sigma}{\sqrt{n}},A_n+2\frac{\sigma}{\sqrt{n}}]$= 95\%-CI for $\mu$.
}

\end{document}